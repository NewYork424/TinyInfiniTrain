<<<<<<< HEAD
# 本地自测

1. 拉取代码仓库 ``git clone git@github.com:InfiniTensor/TinyInfiniTrain.git --recursive``

2. 进入代码目录``cd TinyInfiniTrain``

3. 执行shell脚本下载必要数据``./download_starter_pack.sh``

4. 执行本地编译 ``make`` 或关闭CUDA编译选项进行编译 ``make USE_CUDA=OFF``

5. 运行 ``make test-cpp``，通过所有测例即为完成全部作业。


# 测试文件
全部测试文件以及分值如下。

1. test_elementwise.cc （5分）

验证autograd机制调用Neg kernel的实现，依赖作业一、作业五

2. test_matmul.cc （5分）

验证Matmul kernel的CPU实现，依赖作业二

3. test_matmul_cuda.cc （10分）

验证Matmul kernel的CUDA实现，依赖作业二

4. test_adam.cc （5分）

验证Adam优化器的CPU实现，依赖作业三

5. test_adam_cuda.cc （10分）

验证Adam优化器的CUDA实现，依赖作业三

6. test_tensor.cc （10分）

验证Tensor基础功能，依赖作业四

7. test_dispatcher.cc （20分）

验证多设备分发机制，核心基础设施，依赖作业五

8. test_gpt2.cc （35分）

端到端GPT-2模型测试，依赖所有作业

# 作业题目

## 作业一：autograd机制调用Neg kernel的实现

难度：⭐

对应测例：``TEST(ElementwiseTest, NegForward)``，``TEST(ElementwiseTest, NegBackward)``


需要实现的代码块位置：`infini_train/src/autograd/elementwise.cc`

````c++
std::vector<std::shared_ptr<Tensor>> Neg::Forward(const std::vector<std::shared_ptr<Tensor>> &input_tensors) {
    // =================================== 作业 ===================================
    // TODO：通过Dispatcher获取设备专属kernel，对输入张量进行取反操作
    // HINT: 依赖test_dispatcher，kernel实现已给出
    // =================================== 作业 ===================================
}

std::vector<std::shared_ptr<Tensor>> Neg::Backward(const std::vector<std::shared_ptr<Tensor>> &grad_outputs) {
    // =================================== 作业 ===================================
    // TODO：通过Dispatcher获取设备专属的反向传播kernel，计算梯度
    // HINT: 依赖test_dispatcher，kernel实现已给出
    // =================================== 作业 ===================================
}
````


## 作业二：实现矩阵乘法

难度：⭐⭐

### CPU实现

对应测例：``TEST(MatmulTest, BasicMatrixMultiply)``，``TEST(MatmulTest, BatchedMatrixMultiply)``, ``TEST(MatmulTest, BackwardPass)``

需要实现的代码块位置：`infini_train/src/kernels/cpu/linear.cc`

````c++
    std::shared_ptr<Tensor> MatmulForward(const std::shared_ptr<Tensor> &input, const std::shared_ptr<Tensor> &other) {
        // =================================== 作业 ===================================
        // TODO：实现CPU上的矩阵乘法前向计算
        // REF:
        // =================================== 作业 ===================================
    }

    std::tuple<std::shared_ptr<Tensor>, std::shared_ptr<Tensor>>
        MatmulBackward(const std::shared_ptr<Tensor> &input, const std::shared_ptr<Tensor> &other,
                    const std::shared_ptr<Tensor> &grad_output) {
        // =================================== 作业 ===================================
        // TODO：实现CPU上的矩阵乘法反向传播
        // REF:
        // =================================== 作业 ===================================
    }
````

### CUDA实现

对应测例：``TEST(MatmulTest, BasicMatrixMultiplyCuda)``,``TEST(MatmulTest, BatchedMatrixMultiplyCuda)``,``TEST(MatmulTest, BackwardPassCuda)``

需要实现的代码块位置：`infini_train/src/kernels/cuda/linear.cu`

````c++
    std::shared_ptr<Tensor> MatmulForward(const std::shared_ptr<Tensor> &input, const std::shared_ptr<Tensor> &other) {
        // =================================== 作业 ===================================
        // TODO：实现CUDA上的矩阵乘法前向计算
        // REF:
        // =================================== 作业 ===================================
    }

    std::tuple<std::shared_ptr<Tensor>, std::shared_ptr<Tensor>>
        MatmulBackward(const std::shared_ptr<Tensor> &input, const std::shared_ptr<Tensor> &other,
                    const std::shared_ptr<Tensor> &grad_output) {
        // =================================== 作业 ===================================
        // TODO：实现CUDA上的矩阵乘法反向传播
        // REF:
        // =================================== 作业 ===================================
    }
````

## 作业三：实现Adam优化器

​难度​：⭐

### CPU实现

​对应测例​：``TEST(AdamOptimizerTest, BasicParameterUpdate)``,``TEST(AdamOptimizerTest, MomentumAccumulation)``

​代码位置​：infini_train/src/kernels/cpu/accumulate_grad.cc

````c++
void AdamAccumulateGrad(const std::shared_ptr<Tensor> &grad, const std::shared_ptr<Tensor> &param,
                        const std::shared_ptr<Tensor> &m, const std::shared_ptr<Tensor> &v, float learning_rate,
                        float beta1, float beta2, float eps, int64_t t) {
    // =================================== 作业 ===================================
    // TODO：实现Adam优化器的梯度累积和参数更新
    // REF: 
    // =================================== 作业 ===================================
}
````

### CUDA实现

​对应测例​：``TEST(AdamOptimizerTest, BasicParameterUpdateCuda)``,``TEST(AdamOptimizerTest, MomentumAccumulationCuda)``

​代码位置​：infini_train/src/kernels/cuda/accumulate_grad.cu

````c++
void AdamAccumulateGrad(const std::shared_ptr<Tensor> &grad, const std::shared_ptr<Tensor> &param,
                        const std::shared_ptr<Tensor> &m, const std::shared_ptr<Tensor> &v, float learning_rate,
                        float beta1, float beta2, float eps, int64_t t) {
    // =================================== 作业 ===================================
    // TODO：实现Adam优化器的梯度累积和参数更新
    // REF: 
    // =================================== 作业 ===================================
}
````

## 作业四：实现Tensor基础操作

### 实现Tensor的Flatten操作

​难度​：⭐

​对应测例​：``TEST(TensorTransformTest, Flatten2DTo1D)``,``TEST(TensorTransformTest, FlattenWithRange) ``,``TEST(TensorTransformTest, FlattenNonContiguous)``

​代码位置​：infini_train/src/tensor.cc

````c++
std::shared_ptr<Tensor> Tensor::Flatten(int64_t start, int64_t end) {
    // =================================== 作业 ===================================
    // TODO：实现张量扁平化操作，将指定维度范围[start, end]内的所有维度合并为一个维度
    // HINT: 
    // =================================== 作业 ===================================
}
````

### 实现Tensor的反向传播机制

​难度​：⭐

​对应测例​：``TEST(TensorAutogradTest, BackwardComputesGradient)``,``TEST(TensorAutogradTest, BackwardWithMultipleOutputs)``

​代码位置​：infini_train/src/tensor.cc

````c++
void Tensor::Backward(std::shared_ptr<Tensor> gradient, bool retain_graph, bool create_graph) const {
    // =================================== 作业 ===================================
    // TODO：实现自动微分反向传播
    // 功能描述：1. 计算当前张量对叶子节点的梯度    2. 支持多输出场景的梯度累加
    // HINT: 
    // =================================== 作业 ===================================
}
````


## 作业五 注册算子kernel的实现
​难度​：⭐⭐⭐

​对应测例​：``TEST(DispatcherTest, RegisterAndGetKernel)``,``TEST(DispatcherTest, DuplicateRegistration)``,``TEST(DispatcherTest, GetNonexistentKernel)``

​代码位置​：infini_train/include/dispatcher.h

````c++
template <typename RetT, class... ArgsT> RetT Call(ArgsT... args) const {
    // =================================== 作业 ===================================
    // TODO：实现通用kernel调用接口
    // 功能描述：将存储的函数指针转换为指定类型并调用
    // HINT: 
    // =================================== 作业 ===================================
}

template <typename FuncT> void Register(const KeyT &key, FuncT &&kernel) {
    // =================================== 作业 ===================================
    // TODO：实现kernel注册机制
    // 功能描述：将kernel函数与设备类型、名称绑定
    // =================================== 作业 ===================================
}

#define REGISTER_KERNEL(device, kernel_name, kernel_func) \
    // =================================== 作业 ===================================
    // TODO：实现自动注册宏
    // 功能描述：在全局静态区注册kernel，避免显式初始化代码
    // =================================== 作业 ===================================
````


## 作业六：实现GPT-2整体训练

​难度​：⭐⭐⭐⭐

​对应测例​：``TEST_F(GPT2TrainingTest, LogitsConsistency)``

### 训练过程logits对比

完成以上所有作业，补齐训练框架的所有实现，理论上``TEST_F(GPT2TrainingTest, LogitsConsistency)``可以通过，在用例中判断比较预置的值和单步正向传播计算结果是否在误差允许范围内相等。

### 数据读取实现

​代码位置​：example/common/tiny_shakespeare_dataset.cc

````c++
TinyShakespeareFile ReadTinyShakespeareFile(const std::string &path, size_t sequence_length) {
    /* =================================== 作业 ===================================
       TODO：实现二进制数据集文件解析
       文件格式说明：
    ----------------------------------------------------------------------------------
    | HEADER (1024 bytes)                     | DATA (tokens)                        |
    | magic(4B) | version(4B) | num_toks(4B) | reserved(1012B) | token数据           |
    ----------------------------------------------------------------------------------
       =================================== 作业 =================================== */
}

TinyShakespeareDataset::TinyShakespeareDataset(const std::string &filepath, size_t sequence_length) {
    // =================================== 作业 ===================================
    // TODO：初始化数据集实例
    // HINT: 调用ReadTinyShakespeareFile加载数据文件
    // =================================== 作业 ===================================
}
````

### Tokenizer功能实现

​代码位置​：example/common/tokenizer.cc

````c++
Tokenizer::Tokenizer(const std::string &filepath) {
    /* ===================================== 作业 =====================================
    TODO：实现Tokenizer二进制文件加载

    文件格式说明：
    ----------------------------------------------------------------------------------
    | HEADER (1024 bytes)                     | VOCAB TABLE                           |
    | magic(4B) | version(4B) | vocab_size(4B) | reserved(1012B) | token词表数据       |
    ----------------------------------------------------------------------------------
    ===================================== 作业 ===================================== */
}
````

````c++
std::string Tokenizer::Decode(uint32_t token_id) const {
    /* ===================================== 作业 =====================================
    TODO：实现token_id到文本的转换
    功能描述：根据token_id返回对应的文本片段
    ===================================== 作业 ===================================== */
}
````

````c++
void Tokenizer::GenerateText(infini_train::nn::Module &model, uint32_t batch_size, uint32_t sequence_length,
                             uint32_t text_length, Device device) const {
    /* ...原代码... */
    LOG(INFO) << "start generate text:";
    for (int t = prompt_len; t < text_length; t++) {
        /* ===================================== 作业 =====================================
        TODO：实现单步文本生成逻辑
        HINT：调用model.Forward推理获取logits，根据推理结果进行随机采样，调用Decode获取文本结果
        ===================================== 作业 ===================================== */
    }
    std::cout << std::endl;
}
````
=======
# 特别提示

1. 如果大家在作业项目开发过程中发现项目本身的问题，不要修改用例代码，请及时反馈给助教。

2. 建议大家尽量独立完成作业，完成用例的同时做到"最小化修改"，原则上在作业要求注释的位置开发即可。另外，作业报告内可以写下对代码实现的理解、算法或者系统的示意图以及开发过程中的问题记录，形式不限。

# 本地自测

1. 拉取代码仓库 ``git clone git@github.com:InfiniTensor/TinyInfiniTrain.git --recursive``

2. 进入代码目录``cd TinyInfiniTrain``

3. 执行shell脚本下载必要数据``./download_starter_pack.sh``

4. 执行本地编译 ``make`` 或关闭CUDA编译选项进行编译 ``make USE_CUDA=OFF``

5. 运行 ``make test-cpp``，通过所有测例即为完成全部作业。


# 测试文件
全部测试文件以及分值如下。

1. test_elementwise.cc （5分）

验证autograd机制调用Neg kernel的实现，依赖作业一、作业五

2. test_matmul.cc （5分）

验证Matmul kernel的CPU实现，依赖作业二

3. test_matmul_cuda.cc （10分）

验证Matmul kernel的CUDA实现，依赖作业二

4. test_adam.cc （5分）

验证Adam优化器的CPU实现，依赖作业三

5. test_adam_cuda.cc （10分）

验证Adam优化器的CUDA实现，依赖作业三

6. test_tensor.cc （10分）

验证Tensor基础功能，依赖作业四

7. test_dispatcher.cc （20分）

验证多设备分发机制，核心基础设施，依赖作业五

8. test_gpt2.cc （35分）

端到端GPT-2模型测试，依赖所有作业

# 作业题目

## 作业一：autograd机制调用Neg kernel的实现

难度：⭐

对应测例：``TEST(ElementwiseTest, NegForward)``，``TEST(ElementwiseTest, NegBackward)``


需要实现的代码块位置：`infini_train/src/autograd/elementwise.cc`

````c++
std::vector<std::shared_ptr<Tensor>> Neg::Forward(const std::vector<std::shared_ptr<Tensor>> &input_tensors) {
    // =================================== 作业 ===================================
    // TODO：通过Dispatcher获取设备专属kernel，对输入张量进行取反操作
    // HINT: 依赖test_dispatcher，kernel实现已给出
    // =================================== 作业 ===================================
}

std::vector<std::shared_ptr<Tensor>> Neg::Backward(const std::vector<std::shared_ptr<Tensor>> &grad_outputs) {
    // =================================== 作业 ===================================
    // TODO：通过Dispatcher获取设备专属的反向传播kernel，计算梯度
    // HINT: 依赖test_dispatcher，kernel实现已给出
    // =================================== 作业 ===================================
}
````


## 作业二：实现矩阵乘法

难度：⭐⭐

### CPU实现

对应测例：``TEST(MatmulTest, BasicMatrixMultiply)``，``TEST(MatmulTest, BatchedMatrixMultiply)``, ``TEST(MatmulTest, BackwardPass)``

需要实现的代码块位置：`infini_train/src/kernels/cpu/linear.cc`

````c++
    std::shared_ptr<Tensor> MatmulForward(const std::shared_ptr<Tensor> &input, const std::shared_ptr<Tensor> &other) {
        // =================================== 作业 ===================================
        // TODO：实现CPU上的矩阵乘法前向计算
        // REF:
        // =================================== 作业 ===================================
    }

    std::tuple<std::shared_ptr<Tensor>, std::shared_ptr<Tensor>>
        MatmulBackward(const std::shared_ptr<Tensor> &input, const std::shared_ptr<Tensor> &other,
                    const std::shared_ptr<Tensor> &grad_output) {
        // =================================== 作业 ===================================
        // TODO：实现CPU上的矩阵乘法反向传播
        // REF:
        // =================================== 作业 ===================================
    }
````

### CUDA实现

对应测例：``TEST(MatmulTest, BasicMatrixMultiplyCuda)``,``TEST(MatmulTest, BatchedMatrixMultiplyCuda)``,``TEST(MatmulTest, BackwardPassCuda)``

需要实现的代码块位置：`infini_train/src/kernels/cuda/linear.cu`

````c++
    std::shared_ptr<Tensor> MatmulForward(const std::shared_ptr<Tensor> &input, const std::shared_ptr<Tensor> &other) {
        // =================================== 作业 ===================================
        // TODO：实现CUDA上的矩阵乘法前向计算
        // REF:
        // =================================== 作业 ===================================
    }

    std::tuple<std::shared_ptr<Tensor>, std::shared_ptr<Tensor>>
        MatmulBackward(const std::shared_ptr<Tensor> &input, const std::shared_ptr<Tensor> &other,
                    const std::shared_ptr<Tensor> &grad_output) {
        // =================================== 作业 ===================================
        // TODO：实现CUDA上的矩阵乘法反向传播
        // REF:
        // =================================== 作业 ===================================
    }
````

## 作业三：实现Adam优化器

​难度​：⭐

### CPU实现

​对应测例​：``TEST(AdamOptimizerTest, BasicParameterUpdate)``,``TEST(AdamOptimizerTest, MomentumAccumulation)``

​代码位置​：infini_train/src/kernels/cpu/accumulate_grad.cc

````c++
void AdamAccumulateGrad(const std::shared_ptr<Tensor> &grad, const std::shared_ptr<Tensor> &param,
                        const std::shared_ptr<Tensor> &m, const std::shared_ptr<Tensor> &v, float learning_rate,
                        float beta1, float beta2, float eps, int64_t t) {
    // =================================== 作业 ===================================
    // TODO：实现Adam优化器的梯度累积和参数更新
    // REF: 
    // =================================== 作业 ===================================
}
````

### CUDA实现

​对应测例​：``TEST(AdamOptimizerTest, BasicParameterUpdateCuda)``,``TEST(AdamOptimizerTest, MomentumAccumulationCuda)``

​代码位置​：infini_train/src/kernels/cuda/accumulate_grad.cu

````c++
void AdamAccumulateGrad(const std::shared_ptr<Tensor> &grad, const std::shared_ptr<Tensor> &param,
                        const std::shared_ptr<Tensor> &m, const std::shared_ptr<Tensor> &v, float learning_rate,
                        float beta1, float beta2, float eps, int64_t t) {
    // =================================== 作业 ===================================
    // TODO：实现Adam优化器的梯度累积和参数更新
    // REF: 
    // =================================== 作业 ===================================
}
````

## 作业四：实现Tensor基础操作

### 实现Tensor的Flatten操作

​难度​：⭐

​对应测例​：``TEST(TensorTransformTest, Flatten2DTo1D)``,``TEST(TensorTransformTest, FlattenWithRange) ``,``TEST(TensorTransformTest, FlattenNonContiguous)``

​代码位置​：infini_train/src/tensor.cc

````c++
std::shared_ptr<Tensor> Tensor::Flatten(int64_t start, int64_t end) {
    // =================================== 作业 ===================================
    // TODO：实现张量扁平化操作，将指定维度范围[start, end]内的所有维度合并为一个维度
    // HINT: 
    // =================================== 作业 ===================================
}
````

### 实现Tensor的反向传播机制

​难度​：⭐

​对应测例​：``TEST(TensorAutogradTest, BackwardComputesGradient)``,``TEST(TensorAutogradTest, BackwardWithMultipleOutputs)``

​代码位置​：infini_train/src/tensor.cc

````c++
void Tensor::Backward(std::shared_ptr<Tensor> gradient, bool retain_graph, bool create_graph) const {
    // =================================== 作业 ===================================
    // TODO：实现自动微分反向传播
    // 功能描述：1. 计算当前张量对叶子节点的梯度    2. 支持多输出场景的梯度累加
    // HINT: 
    // =================================== 作业 ===================================
}
````


## 作业五 注册算子kernel的实现
​难度​：⭐⭐⭐

​对应测例​：``TEST(DispatcherTest, RegisterAndGetKernel)``,``TEST(DispatcherTest, DuplicateRegistration)``,``TEST(DispatcherTest, GetNonexistentKernel)``

​代码位置​：infini_train/include/dispatcher.h

````c++
template <typename RetT, class... ArgsT> RetT Call(ArgsT... args) const {
    // =================================== 作业 ===================================
    // TODO：实现通用kernel调用接口
    // 功能描述：将存储的函数指针转换为指定类型并调用
    // HINT: 
    // =================================== 作业 ===================================
}

template <typename FuncT> void Register(const KeyT &key, FuncT &&kernel) {
    // =================================== 作业 ===================================
    // TODO：实现kernel注册机制
    // 功能描述：将kernel函数与设备类型、名称绑定
    // =================================== 作业 ===================================
}

#define REGISTER_KERNEL(device, kernel_name, kernel_func) \
    // =================================== 作业 ===================================
    // TODO：实现自动注册宏
    // 功能描述：在全局静态区注册kernel，避免显式初始化代码
    // =================================== 作业 ===================================
````


## 作业六：实现GPT-2整体训练

​难度​：⭐⭐⭐⭐

​对应测例​：``TEST_F(GPT2TrainingTest, LogitsConsistency)``

### 训练过程logits对比

完成以上所有作业，补齐训练框架的所有实现，理论上``TEST_F(GPT2TrainingTest, LogitsConsistency)``可以通过，在用例中判断比较预置的值和单步正向传播计算结果是否在误差允许范围内相等。

### 数据读取实现

​代码位置​：example/common/tiny_shakespeare_dataset.cc

````c++
TinyShakespeareFile ReadTinyShakespeareFile(const std::string &path, size_t sequence_length) {
    /* =================================== 作业 ===================================
       TODO：实现二进制数据集文件解析
       文件格式说明：
    ----------------------------------------------------------------------------------
    | HEADER (1024 bytes)                     | DATA (tokens)                        |
    | magic(4B) | version(4B) | num_toks(4B) | reserved(1012B) | token数据           |
    ----------------------------------------------------------------------------------
       =================================== 作业 =================================== */
}

TinyShakespeareDataset::TinyShakespeareDataset(const std::string &filepath, size_t sequence_length) {
    // =================================== 作业 ===================================
    // TODO：初始化数据集实例
    // HINT: 调用ReadTinyShakespeareFile加载数据文件
    // =================================== 作业 ===================================
}
````

### Tokenizer功能实现

​代码位置​：example/common/tokenizer.cc

````c++
Tokenizer::Tokenizer(const std::string &filepath) {
    /* ===================================== 作业 =====================================
    TODO：实现Tokenizer二进制文件加载

    文件格式说明：
    ----------------------------------------------------------------------------------
    | HEADER (1024 bytes)                     | VOCAB TABLE                           |
    | magic(4B) | version(4B) | vocab_size(4B) | reserved(1012B) | token词表数据       |
    ----------------------------------------------------------------------------------
    ===================================== 作业 ===================================== */
}
````

````c++
std::string Tokenizer::Decode(uint32_t token_id) const {
    /* ===================================== 作业 =====================================
    TODO：实现token_id到文本的转换
    功能描述：根据token_id返回对应的文本片段
    ===================================== 作业 ===================================== */
}
````

````c++
void Tokenizer::GenerateText(infini_train::nn::Module &model, uint32_t batch_size, uint32_t sequence_length,
                             uint32_t text_length, Device device) const {
    /* ...原代码... */
    LOG(INFO) << "start generate text:";
    for (int t = prompt_len; t < text_length; t++) {
        /* ===================================== 作业 =====================================
        TODO：实现单步文本生成逻辑
        HINT：调用model.Forward推理获取logits，根据推理结果进行随机采样，调用Decode获取文本结果
        ===================================== 作业 ===================================== */
    }
    std::cout << std::endl;
}
````
>>>>>>> f24989af3e66b307a9ff9f6d4e6c9379a9fec88b
